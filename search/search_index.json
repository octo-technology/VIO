{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Visual Inspection Orchestrator is a modular open source framework made to ease the deployment of VI usecases, initiated by Octo Technology.</p> <p>Usecase example: Quality check of a product manufactured on an assembly line.</p> <p>The VIO modules are split between:</p> <p>** vio-edge modules **: The VIO edge modules are deployed close to the object to inspect</p> <ul> <li>The edge orchestrator</li> <li>The edge interface</li> <li>The edge model serving</li> <li>The edge deployment playbook</li> </ul> <p>** vio-hub modules **: The VIO hub modules are deployed in the cloud to collect data and orchestrate the edge fleet</p> <ul> <li>The hub monitoring</li> <li>The hub deployment playbook</li> </ul>"},{"location":"#install-the-framework","title":"Install the framework","text":"<p>To launch the complete stack, you'll need a minima docker install on your machine.</p> <p><code>git clone git@github.com:octo-technology/VIO.git</code></p> <p>Note: The VIO docker images are available here</p>"},{"location":"#run-the-stack","title":"Run the stack","text":"<p>To launch the stack you can use the Makefile on the root of the repository which define the different target based on the docker-compose.yml:</p> <ul> <li>run all edge services (orchestrator, model-serving, interface, db) with local hub monitoring (grafana): <code>make vio-edge-up</code></li> <li>stop and delete all running services: <code>make vio-edge-down</code></li> </ul> <p>In case you want to run a specific module, each module has its own make command:</p> <ul> <li>run the edge_orchestrator containerized: <code>make edge_orchestrator</code></li> <li>run the edge model serving containerized: <code>make edge_model_serving</code></li> <li>run the edge interface containerized: <code>make edge_interface</code></li> </ul> <p>Indeed each of the above target correspond to a command docker-compose.yml. For example, the target <code>edge_orchestrator</code> correspond to :</p> <pre><code>$ docker-compose up -d --build edge_orchestrator\n</code></pre> <p>To check all services are up and running you can run the command <code>docker ps</code>, you should see something like below:</p> <p></p> <p>Once all services are up and running you can access:</p> <ul> <li>the swagger of the edge orchestrator API (OrchestratoAPI): http://localhost:8000/docs</li> <li>the swagger of the edge model serving: http://localhost:8501/docs</li> <li>the hub monitoring: http://localhost:4000/login</li> <li>the edge interface: http://localhost:8080</li> </ul> <p>From the edge interface you can load a configuration and run the trigger button that will trigger the Orchestrator API and launch the following actions:</p> <p></p>"},{"location":"#implementation-example","title":"Implementation example","text":"<p>Here you can find an implementation of VIO deployed on Azure (vio-hub) managing a fleet of Raspberrys (vio-edge):</p> <p></p>"},{"location":"AUTHORS/","title":"Authors","text":"<p>This AI asset was designed by Accenture Technology in collaboration with Octo Technology, if you have any interest please make sure to drop us an email:</p> <pre><code>octo.data-ai.augi@accenture.com\n</code></pre> <p>Members of the team involved into the development of the asset are:</p> <ul> <li>yannick.drant</li> <li>karim.sayadi</li> <li>baptiste.ojeanson</li> <li>louison.roger</li> <li>sofiene.alouini</li> <li>baptiste.saintot</li> </ul>"},{"location":"CICD/","title":"The CICD","text":"<p>We use Github Actions workflows for continuous integration and continuous deployment.</p>"},{"location":"CICD/#the-continuous-integration-workflows","title":"The continuous integration workflows","text":"<ul> <li> <p>ci_edge_interface.yml: the CI of the edge_interface application is decomposed into 2 jobs</p> <pre><code>job lint_and_test_on_edge_interface: static code analysis of JavaScript code (no tests at the moment)\njob build_and_push_images: building the Docker image of the application without publishing to a registry\n</code></pre> </li> <li> <p>ci_edge_orchestrator.yml: the CI of the edge_orchestrator application is decomposed into 2 jobs</p> <pre><code>job lint_and_test_on_edge_orchestrator: static code analysis with Flake8 followed by automated tests (unit, integration, and functional) with storing test reports in Github\njob build_and_push_images: building the Docker image of the application without publishing to a registry\n</code></pre> </li> </ul> <p>The CI workflows (edge_[interface|orchestrator]_ci.yml) are triggered under one of the following conditions: - if a merge request with differences is opened on Github - if a commit on the master branch is pushed to Github</p>"},{"location":"CICD/#the-release-workflows","title":"The release workflows","text":"<ul> <li> <p>publication_vio_images.yml: publication of Docker edge_serving images by manual trigger job</p> <pre><code>build_and_push_images: building Docker images with publishing images to the Github registry\n</code></pre> </li> <li> <p>publication_vio_images_raspberry.yml: publication of Docker edge_serving images by manual trigger job </p> <pre><code>build_and_push_images: building Docker images specific to Raspberry hardware with publishing images to the Github registry\n</code></pre> </li> <li> <p>publication_pages_gh-pages_branch.yml: generation and deployment of documentation</p> </li> </ul> <p>The release workflows are triggered under one of the following conditions: - if a release is created from Github</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contribution-rules","title":"Contribution rules","text":"<ul> <li>The code must be exhaustively tested.</li> <li>Python test package: <code>pytest</code> (with possibility to use unittest mocks)</li> <li>Code style: PEP8</li> <li>Programming language for code and comments: English</li> </ul>"},{"location":"CONTRIBUTING/#coding-conventions","title":"Coding conventions","text":"<ul> <li>120 character max per line</li> <li>Use python 3.6 <code>fstring</code> instead of <code>format()</code> or <code>%s</code></li> <li>Directories, filenames, function and method names in <code>snake_case</code></li> <li>Class names in <code>UpperCamelCase</code></li> <li>Private and protected function and method names prefixed with <code>_</code></li> <li>Please implement private function and method under the corresponding public for more readability</li> <li>Variables name in <code>snake_case</code>, constants <code>MAJ_SNAKE_CASE</code></li> <li>Static method should be used only when needed (for example a method is called without the instance of the   corresponding class).</li> <li>Don't let any dead code. Prefer to create a ticket in the backlog board. Package <code>vulture</code> can be used to check any   existing dead code.</li> <li>Use pathlib instead of native   Python os.path</li> </ul>"},{"location":"CONTRIBUTING/#exception-conventions","title":"Exception conventions","text":"<ul> <li>Create a custom exception in the module <code>exception.py</code> as follow:</li> </ul> <pre><code>class MyCustomException(Exception):\n    pass\n</code></pre> <ul> <li>Add a custom message when it is raised:</li> </ul> <pre><code>if not something:\n    raise MyCustomException('My custom message.')\n</code></pre> <ul> <li>Log the custom exception message when it is caught via the <code>exception</code> method of the logger as follow:</li> </ul> <pre><code>try:\n    do_something()\nexcept MyCustomException as e:\n    self.logger.exception(e)\n    return True\n</code></pre>"},{"location":"CONTRIBUTING/#logging-conventions","title":"Logging conventions","text":"<ul> <li>The logger should always be used at the class level and not the module level.</li> <li>The logger should always be created through the getChild method as a class attribute.</li> <li>The created child logger should always be used in the class.</li> <li>Be aware that all nodes classes that inherit from AbstractNode have already a logger that should be used in the class   scope</li> </ul> <pre><code>from aivi.tools.logger import logger\n\n\nclass MyClassWithLogging:\n    def __init__(self):\n        self.logger = logger.getChild(f'{self.__class__.__name__}')\n        self.attr1 = \"attr1\"\n        self.attr2 = 2\n\n    def my_method(self):\n        do_something()\n        self.logger.info('Doing something!')\n</code></pre>"},{"location":"CONTRIBUTING/#test-conventions","title":"Test conventions","text":"<ul> <li>The same file hierarchy should be used between a project and the associated tests.</li> </ul> <pre><code>my_project\n    |my_project\n    |   |my_module.py\n    |   |\n    |tests\n    |   |test_my_module.py\n    |   |\n</code></pre> <ul> <li>Please respect the matching: one class method for one class (or one function if there is no class).</li> <li>Respect the following syntax (given when then &amp; only one assert):</li> </ul> <pre><code>def my_function(arg):\n    toto = ''\n    if arg:\n        toto += 'a string'\n    return toto\n\n\nclass TestMyFunction:\n    def test_returns_a_string_if_arg_is_true(self):\n        # Given\n        arg = True\n        expected = 'a string'\n\n        # When\n        result = my_function(arg)\n\n        # Then\n        assert result == expected\n\n    def test_returns_empty_string_if_arg_is_none(self):\n        # Given\n        arg = None\n        expected = ''\n\n        # When\n        result = my_function(arg)\n\n        # Then\n        assert result == expected\n</code></pre> <ul> <li>If a mock is needed, use a decorator instead of a context manager:</li> </ul> <pre><code>def my_function(arg):\n    another_function(arg)\n\n\nclass TestMyFunction:\n    @patch('aivi.path.to.module.another_function')\n    def test_returns_a_string_if_arg_is_true(self, mock_another_function):\n        # Given\n        arg = True\n\n        # When\n        my_function(arg)\n\n        # Then\n        mock_another_function.assert_called_once_with(arg)\n</code></pre> <ul> <li>Don't mistake a stub for a mock. A mock is used to assert that it has been called (see above example). A stub   is used to simulate the returned value.</li> </ul>"},{"location":"CONTRIBUTING/#testing-and-docker-images","title":"Testing and docker images","text":"<ul> <li>In order to run the tests, your docker instance will need to be connected to GitHub, allowing docker to pull the   images.   Duplicate the <code>VIO/edge_orchestrator/.env.template</code> file, rename it <code>VIO/edge_orchestrator/.env</code> and fill it with your   GitHub username and access token, to make this connection possible. The token needs <code>read:packages</code>, <code>write:packages</code>   and <code>delete:packages</code> permissions.   More informathion here</li> </ul> <p>\u26a0 If you are not working with a M1 processor \u26a0</p> <p>You may encounter some deployment problems when starting the Orchestrator's tests. To resolve them you will have to build a docker image that fits your system using the Orchestrator's makefile and modify the <code>conftest.py</code> file to edit the <code>image_name</code> field.</p> <pre><code>VIO/edge_orchestrator/tests/conftest.py\n\nEDGE_MODEL_SERVING = {\n    \"image_name\": --NEW_DOCKER_IMAGE--,\n    \"container_volume_path\": \"/tf_serving\",\n    \"host_volume_path_suffix\": \"edge_model_serving\",\n}\n</code></pre> <p>You may need to change the <code>starting_log</code> parameter and remove the call to <code>check_image_presence_or_pull_it_from_registry</code> from the <code>container.py</code> file.</p> <pre><code>VIO/edge_orchestrator/tests/fixtures/containers.py\n\nif tf_serving_host is None or tf_serving_port is None:\n    port_to_expose = 8501\n    container = TfServingContainer(\n        image=image_name,\n        port_to_expose=port_to_expose,\n        env={\"MODEL_NAME\": exposed_model_name},\n        host_volume_path=host_volume_path,\n        container_volume_path=container_volume_path,\n    )\n    container.start(\"INFO:     Application startup complete.\")\n</code></pre>"},{"location":"CONTRIBUTING/#versioning-strategy","title":"Versioning strategy","text":"<ul> <li>Git tutorial:<ul> <li>Basic git tutorial</li> <li>Learn git branching</li> </ul> </li> <li>Naming rules for commits and branches:<ul> <li>One commit per functionality</li> <li>Describe your commits and branches names with a description of the feature</li> <li>How to Write a Git Commit Message</li> <li>Semantic Commit Messages</li> </ul> </li> <li>Semantic versioning : X.Y.Z<ul> <li>X needs to be incremented each time a major change is made (e.g. Python 2 to Python 3)</li> <li>Y needs to be incremented each time a minor change is made (e.g. new features)</li> <li>Z needs to be incremented each time a patch is made (e.g. bug fixing)</li> </ul> </li> <li>Versioning strategy: Trunk Based Development.   To implement a new US, please follow the steps:<ul> <li>Checkout on master and update the branch with the remote repository</li> <li>Create a new branch following the naming convention below and checkout on it:   _feature_description_in_snake_case_starting_with_a_verb (ex: 123_add_users_table) <li>Develop the functionality and don't forget to rebase frequently on master</li> <li>Make sure that all the tests are green</li> <li>Make sure that the <code>README.md</code> doesn't need to be updated</li> <li>Add the files to be tracked and commit your modifications</li> <li>Open a merge request with target <code>master</code> (check the <code>squach commits</code> and <code>remove branch after merge</code> options)</li> <li>The MR must launch a CI pipeline (lint, test and build stage) and this pipeline needs to be green to allow to   merge the MR</li> <li>The final MR commit must follow the following naming conventing: <code>[US-ID] Add my brand new feature</code></li> <li>Organize a review with all your teammates to challenge and validate the code</li> <li>All discussions must be closed before merging</li> <li>After validation: rebase your branch on master, wait for the CI pipeline to be green and merge on   master (<code>fast-forward</code> option activated on gitlab)</li> <p>Reminder of the git commands:</p> <p>git checkout master &amp;&amp; git fetch -p origin &amp;&amp; git reset --hard origin master git checkout -b 123_add_a_new_feature</p>"},{"location":"CONTRIBUTING/#_1","title":"Contributing","text":"<p>git add my_file.py git commit -m '[US-ID] Add my brand new feature' git push origin 123_add_a_new_feature</p>"},{"location":"CONTRIBUTING/#_2","title":"Contributing","text":"<p>git checkout master git pull --rebase origin/master pytest tests</p> <p>git tag (Listing the existing tags) git tag -a  -m  (Create new tag) git push origin  (Sharing the tags to the remote) <p>``` - When to do?     - When you want to add new dependency package.     - When you want to remove existing dependency package.</p>"},{"location":"DOCUMENTATION/","title":"Documentation","text":"<p>To update the documentation, feel free to modify / add markdown file in the <code>/docs</code> folder of the repository</p>"},{"location":"DOCUMENTATION/#preview-locally","title":"Preview Locally","text":"<p>To build locally your github pages site</p> <pre><code>$ mkdocs build\n</code></pre> <p>To test locally your github pages site</p> <pre><code>$ mkdocs serve\n</code></pre>"},{"location":"DOCUMENTATION/#publish-on-github-pages","title":"Publish on github pages","text":"<p>Simply commit your modification on your branch, issue a PR and the workflow publication_pages_gh-pages_branch.yml will be triggered automatically.</p> <p>Note: (Wrong behaviour) to manually push your modification directly to github pages you can execute the command:</p> <pre><code>$ mkdocs gh-deploy\n</code></pre>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"ORGANIZATION/","title":"The Organizational Manifesto","text":"<p>The organization principles of the V.IO project are inspired by selected capabilities of the Accelerate framework, originally presented in the book Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations. The application of these capabilities to the delivery of Machine Learning projects is discussed in a series of articles published on OCTO's blog .</p> <p>The initial version of this organizational manifesto was created following our first organizational workshop. However, it should be continously updated to remain the single source of truth that describes our organization to outsiders and new joiners.</p> <ul> <li> <p>We work in small batches. We are convinced that exceptional value comes from very rapid feedback loops and we want   to be able to track and adjust our trajectory in a short time frame.</p> <ul> <li>The work is organized in 1-week sprints, starting on Wednesday mornings and ending on Tuesday evenings the   following week.</li> <li>A grooming session is organized on Tuesday evenings with a few team members (at least the PO and the Tech Lead).</li> <li>A 30-minutes sprint planning session is organized on Wednesday mornings with all team members in order to recap   the work done over the past sprint and to list the user stories planned for the sprint ahead.</li> </ul> </li> <li> <p>We collect customer feedback. Due to the R&amp;D nature of the project, we are conscious of the risk of disconnection   with the real customer needs during the development phase. As a result, we want to maintain a close relationship with   stakeholders that can act as clients' proxy.</p> <ul> <li>A 30-min demo is organized on Wednesdays every 2 sprints to present the current state of the product to relevant   stakeholders, including subject matter experts and members of OCTO/Accenture leadership.</li> <li>A demonstrator of the edge station will be installed as soon as possible in OCTO's office, where everyone will be   able to interact with the latest version of the continuously deployed system and provide feedback.</li> </ul> </li> <li> <p>We ensure a visibility of the value stream. In addition to the day-to-day tasks, we understand the need for   everyone to have visibility about the flow of product development work.</p> <ul> <li>A link to the V.IO roadmap document presenting the different epics of the project will be shared with everyone and   included in this document.</li> </ul> </li> <li> <p>We encourage team experimentation. We believe that experimenting will allow us to go beyond our opinions and what   we have seen and implemented during our previous experiences and to come up with innovating solutions when dealing   with new issues.</p> <ul> <li>Team members are encouraged to create spikes to explore potential solutions and to share their findings with   everyone during technical work sessions.</li> </ul> </li> <li> <p>We set work-in-progress limits. The objective is to avoid issues that could arise from an overload of the \"Doing\"   section of the project's board.</p> <ul> <li>We set the maximum number of user stories in the \"Doing\" section of the board to: (# of active members of the dev   team / 2).</li> </ul> </li> </ul>"},{"location":"adding_a_custom_model/","title":"Adding a custom model to VIO","text":""},{"location":"adding_a_custom_model/#model-export-vio-configuration","title":"Model export &amp; VIO Configuration","text":""},{"location":"adding_a_custom_model/#model-format","title":"Model format","text":"<p>The edge model serving supports models of 3 types : TensorFlow, TensorFlowLite and Torch.</p> <p>This note will present how to add a custom TensorFlowLite model in VIO. The process is similar for the two other types, for which you can follow the respective ReadMe files (Torch serving and for  TensorFlow serving) and work in their respective edge sub-folder.</p> <p>Comming soon: Integration with Hugging Face </p>"},{"location":"adding_a_custom_model/#saving-the-model","title":"Saving the model","text":"<p>The model has to be given to the Edge_serving module. Export your custom model to tflite and store it as  <code>VIO/edge_model_serving/models/tflite/&lt;model_folder_name&gt;/&lt;model_name&gt;.tflite</code>. (If needed add a .txt file with the  labels/class names)</p> <p>The Edge_orchestrator has to know about the new model that is available. To do so, complete the inventory file  <code>VIO/edge_orchestrator/config/inventory.json</code> with all the information required depending on you model type under the  <code>models</code> category. Note that the model name variable should fit the model folder name. You can refer to this subsection.</p>"},{"location":"adding_a_custom_model/#creating-the-configuration-files","title":"Creating the configuration files","text":"<p>Now that all the components know about your new model, you will need to create a configuration that will use your custom  model. Create a new JSON file in <code>VIO/edge_orchestrator/config/station_configs</code> with any config name. You can follow the configuration of this file in the Add a new configuration subsection.</p>"},{"location":"adding_a_custom_model/#adapting-the-code-to-your-model-optional","title":"Adapting the code to your model - Optional","text":"<p>There are two layers of post-processing that may need to be edited to integrate your model. At the Edge Serving inference  level &amp; for the Edge Orchestrator reception.</p> <ul> <li>Detection model</li> </ul> <p>The implemented methods are designed to support Mobilenet_SSD format, where the output of the model is  <code>List[List[Boxes], List[Classes], List[Scores]]</code> and box format is <code>[ymin, xmin, ymax, xmax]</code>.</p> <p>If your custom model doesn't fit this format, you can add custom post-processing methods.</p> <p>The Edge Serving <code>VIO/edge_model_serving/tflite_serving/src/tflite_serving/api_routes.py</code> calling the model does a first  treatment. Its purpose is separating model's output tensor into a dictionary of the final boxes coordinates, classes and scores.</p> <p>The results are then post processed at the Orchestrator <code>VIO/edge_orchestrator/edge_orchestrator/infrastructure/ model_forward/tf_serving_detection_wrapper.py</code> level to filter the detections of the desired classes and convert the box coordinates to <code>Box: [xmin, ymin, xmax, ymax]</code> then converts the information into a dictionary having a key for each  detection.</p> <ul> <li>Classification &amp; other models</li> </ul> <p>The process is exactly the same as for the detection model, the only difference will be at the Orchestrator level.  Instead of modifying the <code>tf_serving_detection_wrapper.py</code> file select the file that corresponds to your model,  modifying the <code>classification</code> or <code>detection_and_classification</code> wrappers. And you may not have to handle boxes coordinates.</p>"},{"location":"edge_deployment/","title":"Edge Deployment","text":""},{"location":"edge_deployment/#raspberry-setup-raspbian-installation","title":"Raspberry Setup (Raspbian installation)","text":"<p>The Raspberry can be set up thanks to this Makefile.</p> <p>First thing first, insert the SD card in your computer to mount it. Before typing any command, check that the SD card is effectively mounted on <code>/dev/disk2</code>, by typing:</p> <pre><code>$ diskutil list\n</code></pre> <p>Checklist before continuing: - If the SD card is mount on another disk, edit the Makefile by replacing <code>MOUNTING_DIR := /dev/disk2</code> by the disk seen thanks to <code>diskutil</code> command. - If you flash a Raspbian image WITHOUT Desktop ONLY you will need to connect on ssh on the Raspberry. To be able to connect, you need the Raspberry to be in the same local network than your computer. Therefore, setup your WIFI credentials on the Raspberry by editing the wpa_supplicant.conf.template replacing <code>ssid=\"YOUR-NETWORK-SSID\"</code> and <code>psk=\"YOUR-NETWORK-PASSWORD\"</code> values. - If you want to flash a Rasbian image with Desktop, edit the Makefile by replacing:   - <code>RASPIOS := raspios_lite_armhf</code> by <code>raspios_armhf</code> (Raspbian Desktop) or <code>raspios_full_armhf</code> (Raspbian Desktop + recommended Software) and   - <code>RASPIOS_IMAGE_NAME := 2021-05-07-raspios-buster-armhf-lite</code> by an existing image from here or here.</p> <p>Then, type:</p> <pre><code>$ make raspbian\n</code></pre> <p>This command will request information from you and last about 10 minutes. Stay close to your computer until you gave your sudo password (at the beginning of the execution).</p> <p>Then you are all set! With the last command, you just : 1. Formatted the SD card, 2. Downloaded the Raspbian Buster lite image from 2021-05-07, 3. Flashed the image on the SD card, 4. Enabled SSH connection after boot, 5. Setup WIFI credentials and eventually 6. Ejected the SD card.</p> <p>You are now done and can insert the SD card in the Raspberry and make it boot.</p> <p>For more details on what you just have done, see the following parts.</p>"},{"location":"edge_deployment/#optional-step-by-step-raspberry-setup","title":"[OPTIONAL] Step-by-step Raspberry Setup","text":""},{"location":"edge_deployment/#identify-sd-card","title":"Identify SD card","text":"<p>In Terminal, type the following command:</p> <pre><code>$ diskutil list\n</code></pre> <p>You should see something like:  In this case <code>/dev/disk2</code> is my SD card.</p>"},{"location":"edge_deployment/#format-sd-card","title":"Format SD card","text":"<p>To reformat the SD card, go in the deployment directory and type:</p> <pre><code>$ make format-sd-card\n</code></pre> <p>Or:</p> <pre><code>$ diskutil unmountDisk $(MOUNTING_DIR)\n$ diskutil eraseDisk FAT32 $(SD_CARD_NAME) MBRFormat $(MOUNTING_PATH)\n</code></pre> <p>The SD card will be formatted in FAT32 format under the name SDCARD with a Master Boot Record (MBRFormat).</p>"},{"location":"edge_deployment/#verify-formatting","title":"Verify formatting","text":"<p>To check if the formatting was successful, use above command again:</p> <pre><code>$ diskutil list\n</code></pre> <p>Look for a disk named <code>SDCARD</code> like in the following picture: </p>"},{"location":"edge_deployment/#download-the-raspbian-image","title":"Download the Raspbian image","text":"<p>It exists a lot of Raspbian images able to run on Raspberry. Here is an non exhaustive list: - Raspberry Pi OS with desktop and recommended software available here - Raspberry Pi OS with desktop available here - Raspberry Pi OS Lite available here</p> <p>By default, you can download the Raspberry Pi OS Lite from 2021-05-07 by typing:</p> <pre><code>$ make download-raspbian-image\n</code></pre> <p>Or:</p> <pre><code>$ wget https://downloads.raspberrypi.org/$(RASPIOS)/images/$(RASPIOS)-2021-05-28/$(RASPIOS_IMAGE_NAME).zip -O $(IMAGES_DIR)/$(RASPIOS_IMAGE_NAME).zip\n$ unzip $(IMAGES_DIR)/$(RASPIOS_IMAGE_NAME).zip -d $(IMAGES_DIR)/\n</code></pre>"},{"location":"edge_deployment/#flash-an-image","title":"Flash an image","text":"<p>Choose an image and then flash it on the SD card as followed:</p> <pre><code>$ make flash-raspbian-image-on-sd-card\n</code></pre> <p>Or:</p> <pre><code>$ diskutil unmountDisk $(MOUNTING_DIR)\n$ sudo dd if=$(IMAGES_DIR)/$(RASPIOS_IMAGE_NAME).img of=$(MOUNTING_PATH) bs=1024\n</code></pre> <p>To check if the flashing was successful, use above command again:</p> <pre><code>$ diskutil list\n</code></pre> <p>Look for a disk named <code>SDCARD</code> like in the following picture: </p>"},{"location":"edge_deployment/#enable-ssh-and-set-wifi-credentials","title":"Enable SSH and set WIFI credentials","text":"<p>Once formatted, to enable ssh and set your WIFI credentials, first edit <code>ssid=\"YOUR-NETWORK-SSID\"</code> and <code>psk=\"YOUR-NETWORK-PASSWORD\"</code> values in the wpa_supplicant.conf.template.</p> <p>Then, type:</p> <pre><code>$ make setup-wifi-credentials\n</code></pre> <p>It will copy the file named <code>wpa_supplicant.conf.template</code> with the following content (network parameters) on the SD card as <code>wpa_supplicant.conf</code>:</p> <pre><code>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nnetwork={\n    ssid=\"&lt;YOUR-NETWORK-SSID&gt;\"\n    psk=\"&lt;YOUR-NETWORK-PASSWORD&gt;\"\n    key_mgmt=WPA-PSK\n}\n</code></pre> <p>To enable SSH connection after boot, you just need to create an empty file named <code>ssh</code> on your SD card by typing:</p> <pre><code>$ make enable-ssh\n</code></pre> <p>Or:</p> <pre><code>$ touch /Volumes/boot/ssh\n</code></pre>"},{"location":"edge_deployment/#eject-the-sd-card","title":"Eject the SD card","text":"<p>To eject the mounted SD card, type:</p> <pre><code>$ make eject-sd-card\n</code></pre> <p>Or:</p> <pre><code>$ diskutil unmountDisk $(MOUNTING_DIR)\n$ diskutil eject $(MOUNTING_DIR)\n</code></pre>"},{"location":"edge_deployment/#install-and-configure-the-iot-edge-agent-on-raspberrypi","title":"Install and configure the IoT Edge Agent on RaspberryPI","text":"<p>In order to be managed by Azure IoT Hub, each edge device must install an IoT Edge Agent and connect to the Hub.</p> <p>We use Ansible to automate the setup of the IoT Edge Agent.</p>"},{"location":"edge_deployment/#install-ansible-on-the-raspberrypi","title":"Install Ansible on the RaspberryPI","text":"<pre><code>$ pip3 install ansible\n</code></pre>"},{"location":"edge_deployment/#check-ansible-is-correctly-installed","title":"Check Ansible is correctly installed","text":"<pre><code>$ ansible --version\n</code></pre> <p>If <code>ansible</code> command is not found, add <code>/home/pi/.local/bin</code> to the PATH.</p>"},{"location":"edge_deployment/#define-environment-variables","title":"Define environment variables","text":"<p>For The moment, the RaspberryPI connects to the Azure IoT Hub using the connection string of the IoT Edge device identity.</p> <p>We need to provide this connection string through an environment variable.</p> <pre><code>$ export CONNECTION_STRING=\"&lt;primary_connection_string_from_iot_edge_device_on_azure_portal&gt;\"\n</code></pre>"},{"location":"edge_deployment/#execute-the-playbook","title":"Execute the playbook","text":"<p>The following command installs the necessary dependencies, creates the configuration file for the connection to IoT Hub, and applies the configuration.</p> <pre><code>$ cd &lt;path_to_vio_edge_repo&gt;/\n$ ansible-playbook deployment/ansible/install_iot_edge_agent_on_raspberry.yml\n</code></pre>"},{"location":"edge_interface/","title":"Edge Interface","text":"<ol> <li> <p>Select a configuration  </p> </li> <li> <p>Trigger the orchestrator (core)  </p> </li> <li> <p>Review the results  </p> </li> </ol> <p>more documentation coming soon..</p>"},{"location":"edge_model_serving/","title":"Edge Model Serving","text":"<p>more documentation coming soon..</p> <p>You can find the tensorflow serving swagger on the port 8501 of your localhost: http://localhost:8501/docs</p> <p></p>"},{"location":"edge_orchestrator/","title":"Edge Orchestrator","text":"<p>The edge_orchestrator orchestrates the following steps as soon as it is triggered:</p> <ol> <li>image capture</li> <li>image backup</li> <li>metadata backup</li> <li>model inference on images</li> <li>saving results</li> </ol> <p></p>"},{"location":"edge_orchestrator/#set-up-your-development-environment","title":"Set up your development environment","text":"<p>To facilitate the installation of the development environment, a Makefile  automates tasks:</p> <pre><code>$ make\n\u2753 Use `make &lt;target&gt;'\nconda_env                       \ud83d\udc0d Create a Python conda environment\ndependencies                    \u23ec Install development dependencies\ntests                           \u2705 Launch all the tests\nunit_tests                      \u2705 Launch the unit tests\nintegration_tests               \u2705 Launch the integration tests\nfunctional_tests                \u2705 Launch the functional tests\npyramid                         \u2a3a Compute the tests pyramid\npyramid_and_badges              \ud83d\udcdb Generate Gitlab badges\n</code></pre> <p>** Python interpreter installation **</p> <p>The project uses <code>conda</code> to manage Python virtual environments Miniconda installation guide.</p> <p>** Install conda on MacOS **</p> <p>The most direct way to install <code>conda</code> is still Homebrew:</p> <pre><code>brew update\nbrew install --cask miniconda\n</code></pre> <p>** Initialize the project environment **</p> <p>Once Miniconda is installed, create the Python virtual environment and install its dependencies using the Makefile:</p> <pre><code>cd edge_orchestrator\nmake conda_env\n</code></pre> <p>** Install project dependencies **</p> <pre><code>make dependencies\n</code></pre> <p>** Setuptools \"editable mode\" **</p> <p>To be able to benefit from Python packaging without being impacted during local development (i.e. without having to rebuild a package each time it is updated), we use the editable mode (see the official pip doc).</p> <pre><code>pip install -e .\n</code></pre> <p>During the installation of the development environment, the above command will have the following effect:</p> <p>A file edge_orchestrator.egg-link was created in the edge_orchestrator virtual environment with the following content:</p> <pre><code>cat /usr/local/Caskroom/miniconda/base/envs/edge_orchestrator/lib/python3.9/site-packages/edge_orchestrator.egg-link\n/path/to/project/sources/vio_edge/edge_orchestrator\n</code></pre> <p>Thus, thanks to the egg-link, the python module edge_orchestrator is properly installed as a library in the virtual environment, but does not require regular repackaging after an update in local.</p> <p>** Setuptools \"development mode\" **</p> <p>To be able to install the library and its development dependencies (test libraries):</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>** Setuptools \"console_scripts\" EntryPoints **</p> <p>In the edge_orchestrator.egg-link file of the edge_orchestrator, the following entry_points block is configured</p> <pre><code>setup(\n    name=\"edge_orchestrator\",\n    # [...]\n   entry_points={\n      'console_scripts': [\n         'edge_orchestrator = edge_orchestrator.__main__:main',\n      ],\n   },\n)\n</code></pre> <p>The <code>setuptools</code> package allows you to configure different types of scripts, including console_scripts, which will generate a \"shim\" shell script that will be placed on the PATH and will call the edge_orchestrator.main:main function as configured.</p> <p>This edge_orchestrator edge_orchestrator  is located in the virtual environment created during project installation.</p> <p>When the virtual environment is activated (by running <code>conda</code> activate edge_orchestrator), the $PATH environment variable is configured to point to the bin/ folder of the virtual environment.</p> <pre><code>$ echo $PATH\n/usr/local/Caskroom/miniconda/base/envs/edge_orchestrator/bin:[...]\n</code></pre> <p>If we look inside this script, we notice that it is responsible for importing our edge_orchestrator module and calling its entry point.</p> <pre><code>#!/usr/local/Caskroom/miniconda/base/envs/edge_orchestrator/bin/python3.9\n# EASY-INSTALL-ENTRY-SCRIPT: 'edge_orchestrator','console_scripts','edge_orchestrator'\nimport re\nimport sys\n\n# for compatibility with easy_install; see #2198\n__requires__ = 'edge_orchestrator'\n\nfrom pkg_resources import load_entry_point\n\n[...]\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(load_entry_point('edge_orchestrator', 'console_scripts', 'edge_orchestrator')())\n</code></pre> <p>For more information, the documentation can be found here.</p>"},{"location":"edge_orchestrator/#tests","title":"Tests","text":"<p>To run all tests:</p> <pre><code>make tests\n</code></pre> <p>To run only unit tests:</p> <pre><code>make unit_tests\n</code></pre>"},{"location":"edge_orchestrator/#api-routes","title":"API Routes","text":"<p>All routes are prefixed with api/v1. For example, to retrieve the list of items locally, use this url:  http://localhost:8000/api/v1/items</p> <p>You can also refer to the API swagger on the /docs url: http://localhost:8000/docs</p>"},{"location":"edge_orchestrator/#add-a-new-configuration","title":"Add a new configuration","text":"<p>All the JSON config files are in <code>edge_orchestrator/config/station_configs</code> If you want to create a new config, you need to add a new JSON in the above directory. Here's a template of a config file.</p> <pre><code>{\n  \"cameras\": {\n    \"camera_id3\": {\n      \"type\": \"fake\" #type of the camera : fake, pi_camera or usb_camera\n      \"input_images_folder\": \"people_dataset\",\n      \"position\": \"front\",\n      \"exposition\": 100,\n      \"models_graph\": {\n        \"model_id1\": {\n          \"name\": \"mobilenet_ssd_v2_coco\", #name of the model\n          \"depends_on\": [], #if this model depends on another model, if none then empty list\n          \"class_to_detect\": [\"cell phone\"] #class to detect, always in list format and only for object detection model. If classification model, can delete this row\n        },\n        \"model_id6\": {\n          \"metadata\": \"cellphone_connection_control\",#name of the model\n          \"depends_on\": [ \n            \"model_id1\"\n          ], #the model_id6 depends on the model_id1\n          \"class_to_detect\": [\"connected\"]\n        }\n      },\n      \"camera_rule\": {\n        \"name\": \"min_nb_objects_rule\", #name of the camera rule\n        \"parameters\": {\n          \"class_to_detect\": [\"person\"], #always a list\n          \"min_threshold\": 1\n        }\n      }\n    },\n    \"camera_id2\": { # if there's another camera, if not you can delete this section, if there's a third then add one.\n      \"type\": \"fake\",\n      \"input_images_folder\": \"people_dataset\",\n      \"position\": \"front\",\n      \"exposition\": 100,\n      \"models_graph\": {\n        \"model_id1\": {\n          \"name\": \"mobilenet_ssd_v2_face\",\n          \"depends_on\": [],\n          \"class_to_detect\": [\"face\"]\n        }\n      },\n      \"camera_rule\": {\n        \"name\": \"min_nb_objects_rule\",\n        \"parameters\": {\n          \"class_to_detect\": [\"face\"],\n          \"min_threshold\": 1\n        }\n      }\n    }\n  },\n  \"item_rule\": {\n    \"name\": \"min_threshold_ko_rule\", #the item rule name\n    \"parameters\": {\n      \"threshold\": 1\n    }\n  }\n}\n</code></pre> <p>The comments are only here to guide you, you should delete them in your new json config.</p> Station config : Camera Description <code>type</code> Camera type can be <code>fake</code>, <code>pi_camera</code> and <code>usb_camera</code>. <code>pi_camera</code> will be used for raspberry deployment. <code>usb_camera</code> is used when it is required to find a camera or webcam connected to the edge. A <code>fake</code> camera will not capture image but pick a random .jpg or .png file in the folder pointed by the \"input_images_folder\" parameter, which will be located in edge_orchestrator/data/. <code>input_images_folder</code> Used with <code>fake</code> cameras, is the path to the folder from which the pictures are taken. <code>position</code> Used for metadata, purpose of saving the camera parameters in the future <code>exposition</code> Used for metadata, purpose of saving the camera parameters in the future <code>models_graph</code> Pipeline of models used during inference. Dictionary of models, containing their names, depencecies to other models and all its possible parameters. <code>camera_rule</code> Dictionary, key <code>name</code> containing the rule name and key <code>parameters</code> containing the selected rule's inputs <p>For the item rules, just inform the rule's <code>name</code> and <code>parameters</code> as a dictionary of the inputs.</p>"},{"location":"edge_orchestrator/#add-a-new-model","title":"Add a new model","text":"<ul> <li> <p>All our models are in tflite format. In order to add an already trained model in the <code>flite_serving</code> folder.  Inside this folder should be the .tflite model and if needed a .txt file with the labels/class names.</p> </li> <li> <p>You also need to add this model in the inventory located in <code>edge_orchestrator/config/inventory.json</code> under the  <code>models</code> category. </p> </li> <li>Classification model     <code>\"your_new_model_name\": {       \"category\": \"classification\",       \"version\": 1,       \"class_names\": [         \"class name 1\",         \"class name 2\",          ...       ],       \"image_resolution\": [         x resolution for your trained model (int),         y resolution for your trained model (int)       ]     }</code></li> <li> <p>Object detection model     ```     \"your_new_model_name\": {       \"category\": \"object_detection\",       \"version\": 1,       \"class_names_path\": \"{name of file with the class names}.txt\",       \"output\": {         \"boxes_coordinates\": \"{name of the boxes_coordinates variable in your model}\",         \"objectness_scores\": \"{name of the objectness_scores variable in your model}\",         \"number_of_boxes\": \"{name of the number_of_boxes variable in your model}\",         \"detection_classes\": \"{name of the detection_classes variable in your model}\"       },       \"image_resolution\": [         x resolution for your trained model (int),         y resolution for your trained model (int)       ],       \"objectness_threshold\": minimum threshold score for an object to be detected (float)</p> <p>} ```</p> </li> </ul> Model parameters Description <code>category</code> Model's category, can be <code>object_detection</code>, <code>classification</code> or <code>object_detection_with_classification</code> <code>version</code> Model's version, used in the API link, should be 1 mais c'est pas utilis\u00e9 <code>model_type</code> Type of model used, is <code>Mobilenet</code> or <code>yolo</code>. Mobilenet models return boxes as [ymin, xmin, ymax, xmax] and Yolo as [x_center, y_center, width, height] <code>image_resolution</code> List of ints corresponding to the x.y image size ingested by the model <code>depends_on</code> Used to design model pipelines, is a list of models' names <code>class_names</code> List of the label names as a list of strings <code>class_names_path</code> Path to the labels files, the file should be located under the <code>edge_orchestrator/data</code> folder <code>class_to_detect</code> List of label names that will be detected (for Mobilenet) <code>output: detection_boxes</code> For detection models, name which will be given to the predicted boxes <code>output: detection_scores</code> For detection models, name which will be given to the predicted scores <code>output: detection_classes</code> For detection models, name which will be given to the predicted classes <code>output: detection_metadata</code> For detection models, name which will be given to the predicted metadata <code>objectness_threshold</code> Score threshold under which an object won't be detected"},{"location":"edge_orchestrator/#add-new-camera-rule","title":"Add new camera rule","text":"<p>In order to make a final decision i.e the item rule, we first need camera rules. Each camera gets a rule. - Each rule is in a distinct file located in  <code>edge_orchestrator/edge_orchestrator/domain/model/business_rules/camera_business_rules</code> It's in this method that's the camera rule will be described.  The method only takes the inference in argument.</p> <ul> <li>You also need to precise the camera rule in the station config in <code>edge_orchestrator/config/station_configs/</code></li> </ul> <pre><code>\"camera_rule\": {\n        \"name\": \"name of the rule\",\n        \"parameters\": {\n          # parameters of the rules for example : \n          \"expected_label\": [\"connected\"]\n        }\n      }\n\n</code></pre> <ul> <li>You need to add the new rule in the <code>get_camera_rule</code> function located in <code>edge_orchestrator/edge_orchestrator/domain/model/camera.py</code> which get the good method from the name of the camera rule in the station config file. </li> </ul>"},{"location":"edge_orchestrator/#add-new-item-rule","title":"Add new item rule","text":"<p>This is to make a final decision i.e the item rule. Each station config gets an item rule (only one). - Each rule is in a distinct file located in  <code>edge_orchestrator/edge_orchestrator/domain/model/business_rules/item_business_rules</code> It's in this method that's the item rule will be described.  The method only takes the camera decisions in argument.</p> <ul> <li>You also need to precise the item rule in the station config in <code>edge_orchestrator/config/station_configs/</code></li> </ul> <pre><code>  \"item_rule\": {\n    \"name\": \"name of the item rule\",\n    \"parameters\": {\n    # parameters of the rules for example : \n      \"threshold\": 1\n    }\n  }\n\n</code></pre> <ul> <li>You need to add the new rule in the <code>get_item_rule</code> function located in <code>edge_orchestrator/edge_orchestrator/domain/model/item.py</code> which get the good method from the name of the item rule in the station config file. </li> </ul> <p>The camera and item rules are called in the edge_orchestrator method <code>edge_orchestrator/edge_orchestrator/domain/use_cases/edge_orchestrator.py</code>  in the <code>apply_business_rules</code> function.</p>"},{"location":"edge_orchestrator/#adapters-description","title":"Adapters description","text":""},{"location":"edge_orchestrator/#binary-storage-adapter","title":"Binary storage adapter","text":"<p>When an image is captured by any camera, VIO is saving the image in a storage. The binary storage adapter is responsible this process. 4 binary storage systems are implemented in VIO: - File System Binary Storage: Saves the image in the filesystem under the <code>VIO/edge_orchestrator/data/storage</code> folder. - Memory Binary Storage: Saves the image in memory as a dictionary. - Azure Container Binary Storage: Saves the images in an Azure Blob Storage container. - GCP Binary Storage: Saves the images in a Google Cloud Storage bucket.</p> <p>Theses adapters are implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/binary_storage</code> folder and  the base mock class is defined <code>edge_orchestrator/edge_orchestrator/domain/ports/binary_storage.py</code>.</p>"},{"location":"edge_orchestrator/#camera-adapter","title":"Camera adapter","text":"<p>The camera adapter is responsible for localizing the connected cameras and capturing images, 3 camera systems are  implemented in VIO and are chosen in the model configuration: - Fake Camera: Picks a random .jpg or .png file in the folder pointed by the \"input_images_folder\" parameter,  which will be located in edge_orchestrator/data/. - Pi Camera: Used for Raspberry deployments to capture the images. - USB Camera: Used when to find the connected cameras or webcams and capture images. <p>Theses adapters are implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/camera</code> folder and the base  Camera class from which the adapters inherit is defined in <code>edge_orchestrator/edge_orchestrator/domain/models/camera.py</code>.</p>"},{"location":"edge_orchestrator/#inventory-adapter","title":"Inventory adapter","text":"<p>Used to store the configuration settings. One adapter is available for json configuration files. - Json Inventory: Reads the configuration from a json file.</p> <p>This adapter is implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/inventory</code> folder and the base mock Inventory class is defined in <code>edge_orchestrator/edge_orchestrator/domain/ports/inventory.py</code>.</p>"},{"location":"edge_orchestrator/#metadata-storage-adapter","title":"Metadata storage adapter","text":"<p>When a task is done, the configuration and the results are saved in a metadata storage. An example of the stored data is shown below:</p> Metadata json<p> <pre><code>\"serial_number\": \"serial_number\",\n    \"category\": \"category\",\n    \"station_config\": \"yolo_coco_nano_with_1_fake_camera\",\n    \"cameras\": {\n      \"camera_id4\": {\n        \"brightness\": null,\n        \"exposition\": 100,\n        \"position\": \"back\",\n        \"source\": \"people_dataset\"\n      }\n    },\n    \"received_time\": \"2024-04-02 11:22:12\",\n    \"inferences\": {\n      \"camera_id4\": {\n        \"model_id4\": {\n          \"object_1\": {\n            \"label\": \"person\",\n            \"location\": [\n              0.2731,\n              0.1679,\n              0.5308,\n              0.9438\n            ],\n            \"score\": 0.9098637104034424,\n            \"metadata\": null\n          },\n          \"object_2\": {\n            \"label\": \"person\",\n            \"location\": [\n              0.1099,\n              0.351,\n              0.2252,\n              0.6945\n            ],\n            \"score\": 0.559946596622467,\n            \"metadata\": null\n          }\n        }\n      }\n    },\n    \"decision\": \"OK\",\n    \"state\": \"Done\",\n    \"error\": null,\n    \"id\": \"03a7adc7-59d5-4190-8160-4a71fd07cac5\"\n</code></pre> </p> <p>4 metadata storage systems are implemented in VIO: - File System Metadata Storage: Saves the metadata in the filesystem under the <code>VIO/edge_orchestrator/edge_orchestrator/data/storage</code> folder. - Memory Metadata Storage: Saves the metadata in memory as a dictionary. - Azure Container Metadata Storage: Saves the metadata in an Azure Blob Storage container. - GCP Metadata Storage: Saves the metadata in a Google Cloud Bucket. - MongoDB Metadata Storage: Saves the metadata in a MongoDB database.</p> <p>Theses adapters are implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/metadata_storage</code> folder and  the base mock class is defined <code>edge_orchestrator/edge_orchestrator/domain/ports/metadata_storage.py</code>.</p>"},{"location":"edge_orchestrator/#model-forward-adapter","title":"Model forward adapter","text":"<p>The model forward adapter is responsible for the model inference, it performs the inference with the required post and  pre-processing. 5 model forward systems are implemented in VIO: - Fake Model Forward: Returns a random inference result. - TF Serving Wrapper: Redirect the prediction task to one of the 3 following Tensor Flow model forwarders. - TF Serving Detection Wrapper: Performs the inference with a detection model. - TF Serving Classification Wrapper: Performs the inference with a classification model. - TF Serving Detection and Classification Wrapper: Performs the inference with a detection and classification model.</p> <p>Theses adapters are implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/model_forward</code> folder and the base mock class is defined <code>edge_orchestrator/edge_orchestrator/domain/ports/model_forward.py</code>.</p>"},{"location":"edge_orchestrator/#station-config-adapter","title":"Station config adapter","text":"<p>Used to store the station configuration settings. One adapter is available for json configuration files.</p> <p>This adapter is implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/station_config</code> folder and the base mock StationConfig class is defined in <code>edge_orchestrator/edge_orchestrator/domain/ports/station_config.py</code>.</p>"},{"location":"edge_orchestrator/#telemetry-sink-adapter","title":"Telemetry sink adapter","text":"<p>Sends the telemetry data to a sink for further processing and analysis. 3 telemetry sink systems are implemented in VIO: - Fake Telemetry Sink: Does nothing. - Azure Telemetry Sink: Sends the telemetry data to an Azure IoT Hub Module. - Postgresql Telemetry Sink: Sends the telemetry data to a Postgresql database.</p> <p>Theses adapters are implemented in the <code>edge_orchestrator/edge_orchestrator/infrastructure/telemetry_sink</code> folder and the base mock class is defined <code>edge_orchestrator/edge_orchestrator/domain/ports/telemetry_sink.py</code>.</p>"},{"location":"hub_deployment/","title":"Hub Deployment","text":"<p>The VIO hub modules can be deployed in any cloud, for this tutorial we decided to use Azure and its IoT solution Azure IoT Edge/Hub</p> <p>This section allows you to create all the Azure infrastructure for VIO: - Storage resources (Storage Account + PostgreSQL) - The IoT Hub - An Azure function (<code>telemetry_saver</code>) to save Device-to-Cloud telemetry data in PostgreSQL - An Event Grid Topic to connect IoT Hub with the <code>telemetry_saver</code> Azure function</p>"},{"location":"hub_deployment/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, you need to install Ansible and its dependencies for Azure and PostgreSQL.</p> <pre><code>$ cd ./deployment/\n$ conda create -n ansible python=3\n$ conda activate ansible\n$ pip install -r requirements.txt\n$ ansible-galaxy collection install azure.azcollection\n$ ansible-galaxy collection install community.grafana\n</code></pre> <p>You'll also need : - The Azure CLI - The Azure CLI IoT extension extension - The Azure Functions Core Tools</p> <p>On MacOS, these can be installed as follows:</p> <pre><code>$ brew update\n$ brew install azure-cli\n$ az extension add --name azure-iot\n$ brew tap azure/functions\n$ brew install azure-functions-core-tools@3\n</code></pre> <p>Once you have installed <code>azure-cli</code>, you can login to Azure using your Accenture account:</p> <pre><code>$ az login\n</code></pre> <p>Make sure you are using the Azure subscription <code>IX-Visual-Inspection-MDI</code>. You can check that with:</p> <pre><code>$ az account list --output table\n</code></pre> <p>If <code>IX-Visual-Inspection-MDI</code> is not the default subscription, you can switch to it with the following command:</p> <pre><code>$ az account set --subscription \"IX-Visual-Inspection-MDI\"\n</code></pre>"},{"location":"hub_deployment/#define-the-mandatory-environment-variables","title":"Define the mandatory environment variables","text":"<p>In order to create and configure all the Azure infrastructure, we need to define some environment variables: </p> <pre><code>$ export REGISTRY_USERNAME=&lt;your_gitlab_username&gt;\n$ export REGISTRY_PASSWORD=&lt;your_gitlab_container_registry_token&gt;\n$ export POSTGRES_USERNAME=&lt;postgres_admin_username&gt;\n$ export POSTGRES_PASSWORD=&lt;postgres_admin_password&gt;\n$ export AZURE_STORAGE_CONNECTION_STRING=&lt;azure_storage_connection_string&gt;\n</code></pre>"},{"location":"hub_deployment/#registry-username","title":"Registry Username","text":"<p>firstname.lastname (i.e nicolas.dupont)</p>"},{"location":"hub_deployment/#registry-password","title":"Registry Password","text":"<p>You can find it on gitlab, click on your profil picture (top right corner) --&gt; preferences.  On the <code>Access Tokens</code> category, you can generate a token. I suggest no expiration date, and you select all the scopes. Keep this token safe, once it's generated you cannot retrieve it on gitlab anymore.</p>"},{"location":"hub_deployment/#postgres-username-and-password","title":"Postgres Username and Password.","text":"<ul> <li>To Get the Postgres username, go the Azure Portal, our subscription <code>IX-Visual-Inspection-MDI</code> --&gt; our resource group <code>vio-rg-dev</code> --&gt; the <code>vio-function-app-dev</code> function app. On the left side bar menu, click on <code>Configuration</code> and unhide the POSTGRES_USER field. You only need what's before the @. Here it's <code>vioadmin</code></li> <li>To Ge the Postgres password, it's on the same page but unhide the POSTGRES_PASSWORD field.</li> </ul>"},{"location":"hub_deployment/#create-azure-infrastructure","title":"Create Azure Infrastructure","text":"<p>The following command creates all the Azure IoT infrastructure for VIO.</p> <pre><code>$ ansible-playbook ansible/create_azure_cloud_infrastructure.yml -e 'ansible_python_interpreter=&lt;path_to_your_conda_env_python_interpreter&gt;'\n</code></pre>"},{"location":"hub_deployment/#deploy-hub-monitoring-grafana","title":"Deploy hub monitoring (grafana)","text":"<p>To deploy Grafana dashboard and data-sources, run the following playbook :</p> <pre><code>$ ansible-playbook -i ansible/inventory/production.ini ansible/update_grafana_dashboard.yml --ask-pass\n</code></pre> <p>This will copy the files on the Grafana resource deployed in Azure and relaunch the grafana service to take into account the brand uploaded files. </p>"},{"location":"hub_monitoring/","title":"Hub Monitoring","text":"<p>The monitoring is here to help us monitor our IoTHub Devices and Modules via a Grafana dashboard.</p>"},{"location":"hub_monitoring/#dashboards","title":"Dashboards","text":"<p>Once you've created a dashboard in Grafana you can export it in JSON and then save it in the <code>monitoring/dashboard</code>folder.  This manipulation allows you to save your dashboard and visualize it when you deploy your Grafana. - In Grafana, you can directly export your dashboard via the JSON file. To do this, you open your Grafana, then go to Dashboards/Manage and then click on Import and import your JSON file.</p> <p>Manage Dashboards: </p> <p></p> <p>To import a dashboard from a JSON file : </p> <p></p>"},{"location":"hub_monitoring/#provisioning","title":"Provisioning","text":""},{"location":"hub_monitoring/#dashboards_1","title":"Dashboards","text":"<p>You can manage dashboards in Grafana by adding one or more YAML config files in the provisioning/dashboards directory. Each config file can contain a list of dashboards providers that load dashboards into Grafana from the local filesystem. When Grafana starts, it will update/insert all dashboards available in the configured path</p>"},{"location":"hub_monitoring/#datasources","title":"Datasources","text":"<p>It\u2019s possible to manage data sources in Grafana by adding one or more YAML config files in the provisioning/datasources directory. Each config file can contain a list of datasources that will get added or updated during start up. If the data source already exists, then Grafana updates it to match the configuration file. The config file can also contain a list of data sources that should be deleted. That list is called deleteDatasources. Grafana will delete data sources listed in deleteDatasources before inserting/updating those in the datasource list. Our datasources are protected by tokens.   - In order to get the iothub_metrics datasource token you must run the command :  <code>az account get-access-token -s {subscription-id}</code> in your terminal.     You will have a Bearer token that you need to pass into the field <code>httpHeaderValue1</code> in the yaml datasource file preceded by the keyword 'Bearer' - In order to get the iothub_devices datasource token you must run the command : <code>az iot hub generate-sas-token -n {your-iothub-name}</code> in your terminal. This command returns you a sas token that you need to copy in the field <code>httpHeaderValue1</code> in the yaml datasource file</p>"},{"location":"hub_monitoring/#deployment","title":"Deployment","text":""},{"location":"hub_monitoring/#prerequisites","title":"Prerequisites","text":"<p>install sshpass,     - on mac:      <code>brew install hudochenkov/sshpass/sshpass</code>     - on linux     <code>apt-get install sshpass</code></p>"},{"location":"hub_monitoring/#deploy-the-grafana-in-azure","title":"Deploy the Grafana in Azure","text":"<p>To deploy your Grafana on Azure, launch <code>make deploy-grafana-azure</code>  As our datasource tokens are only available for a short period of time (2h), you'll need to often redeploy the grafana (to have new tokens) or you'll face some authorization issues in the dashboard.  </p>"},{"location":"hub_monitoring/#access-to-grafana","title":"Access to Grafana","text":"<p>The grafana dashboard is public. In order to access it you need its public IP address and its port. The port is 3000. To get the public IP address, to you must go to the Azure Portal --&gt; our subscription <code>IX-Visual-Inspection-MDI</code> --&gt; our resource group <code>vio-rg-dev</code> --&gt; <code>grafana</code> (Virtual machine). You can copy the Public IP Address</p> <p></p> <p>To have access to the Grafana, you fill the address bar of your navigator with <code>Public IP Adress:Port</code>. Here it's 51.11.242.209:3000</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#mlops-framework","title":"MLOPS framework","text":"<p>VIO framework propose a generic code base for each of the following MLOPS features:</p> <ul> <li>The data gathering</li> <li>The model monitoring</li> <li>The model factory</li> <li>The fleet management</li> <li>The software factory</li> </ul> <p></p>"},{"location":"overview/#modular-framework","title":"Modular framework","text":"<p>VIO core has been built following the hexagonal architecture patterns, therefore it can be adapted to its production environement constraints (cloud provider, hardware, ML framework...).</p> <p></p>"},{"location":"overview/#micro-services-approach","title":"Micro-services approach","text":"<p>Each sub folders below are indeed a module, an application, an independant micro service. Anyone of them is therefore functional by itself.</p>"},{"location":"overview/#vio-edge-modules","title":"vio-edge modules","text":"<ul> <li>The edge orchestrator</li> <li>The edge interface</li> <li>The edge model serving</li> <li>The edge deployment playbook</li> </ul>"},{"location":"overview/#vio-hub-modules","title":"vio-hub modules","text":"<ul> <li>The hub monitoring</li> <li>The hub deployment playbook</li> </ul> <p>All of those modules have been packages inside a dedicated docker images to facilitate their deployment.</p>"},{"location":"running_vio_locally/","title":"Running VIO locally","text":"<p>In order to use VIO locally we are going to start 3 modules in 3 different terminals. Some of them will need conda installed. The most direct way to install conda on MacOS is via Homebrew:</p> <pre><code>brew update\nbrew install --cask miniconda\n</code></pre>"},{"location":"running_vio_locally/#running-the-edge-model-serving","title":"Running the edge model serving","text":"<p>The edge model serving is the module that is going to do the inference computing using the stored models (it does the <code>.predict()</code>\"). It is called by the edge orchestrator.</p> <p>You can follow the conda environment installation from the  edge model serving's ReadMe file. Once it is done you can start the  server using the make command.</p> <pre><code>make run_tflite_serving\n</code></pre>"},{"location":"running_vio_locally/#running-the-edge-orchestrator","title":"Running the edge orchestrator","text":"<p>The edge orchestrator will administrate the configuration, images captures, storage and communication with the edge models for inference then applying business rules. The following commands will create a package of the orchestrator environment as described here</p> <pre><code>cd edge_orchestrator\nmake conda_env\nmake install\npip install -e .[dev]\n</code></pre> <p>It is now required to edit the configured environment according to a local run. In the <code>VIO/edge_orchestrator/edge_orchestrator/api_config.py</code> file, inform the local profile on line 7.</p> <pre><code>def load_config():\n    configuration = os.environ.get(\"API_CONFIG\", \"local\")\n</code></pre> <p>Now start the server :</p> <pre><code>python -m edge_orchestrator\n</code></pre>"},{"location":"running_vio_locally/#running-the-interface","title":"Running the interface","text":"<p>The interface is connected to the edge model serving, facilitating its usage.</p> <p>You can follow the Edge Interface's ReadMe file commands to run this part.</p>"}]}